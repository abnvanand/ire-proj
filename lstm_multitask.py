# -*- coding: utf-8 -*-
"""LSTM_Multitask.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/153c8A3RwoUgJwBHnoc6CapWzmSzw1huM

CuDNNLSTM() trains faster but does not allow Dropout() 
so falling back to LSTM() adding dropout layers we achieved validation accuracy of 93%
"""

# from google.colab import drive
# drive.mount("/content/drive", force_remount=True)
import os

DATASET_PATH = "/content/drive/My Drive/ire-proj/processedData"
# !ls "$DATASET_PATH"

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--datasetdir', '-d', default="processedData")
    parser.add_argument('--trainingfile', '-r', default="processedData")
    parser.add_argument('--testfile', '-e', default="processedData")

    args = parser.parse_args()
    DATASET_PATH = args.datasetdir
    TRAINING_FILE = args.trainingfile
    TEST_FILE = args.testfile

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

from keras import Sequential, Model, Input
from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, Flatten, Dense, \
    GlobalAveragePooling1D, Dropout, LSTM, CuDNNLSTM, RNN, SimpleRNN, Conv2D, GlobalMaxPooling1D
from keras import callbacks

import re

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.layers import Embedding
from keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
import pickle

import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.metrics import classification_report

from keras.callbacks import ModelCheckpoint
from keras.models import load_model
from numpy.testing import assert_allclose

N_TRAINING_SAMPLES = 10

N_TEST_SAMPLES = N_TRAINING_SAMPLES // 2 if N_TRAINING_SAMPLES is not None else None

"""# Preparing text data
Format text samples and labels into tensors that can be fed into a neural network.
- keras.preprocessing.text.Tokenizer
- keras.preprocessing.sequence.pad_sequences
"""

# Source: https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html

MAX_NUM_WORDS = 40  # dictionary size
MAX_SEQUENCE_LENGTH = 15  # max word length of each individual article
EMBEDDING_DIM = 300  # dimensionality of the embedding vector (50, 100, 200, 300)

TOKENIZER_DUMP_FILE = 'tokenizer.p'

tokenizer = Tokenizer(num_words=MAX_NUM_WORDS,
                      filters='!"#$%&()*+,-./:;<=>?@[\]^_`{|}~')


def tokenize_trainingdata(X):
    tokenizer.fit_on_texts(X)

    sequences = tokenizer.texts_to_sequences(X)

    word_index = tokenizer.word_index
    print(f'Found {len(word_index)} unique tokens.')

    X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

    with open(TOKENIZER_DUMP_FILE, 'wb') as fp:
        pickle.dump(tokenizer, fp)

    return X, word_index


def tokenize_testdata(X):
    with open(TOKENIZER_DUMP_FILE, 'rb') as fp:
        tokenizer = pickle.load(fp)

    print(f'Found {len(tokenizer.word_index)} unique tokens.')

    sequences = tokenizer.texts_to_sequences(X)

    X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)

    return X


def reverse_to_categorical(y):
    return np.argmax(y[:5], axis=1)


"""# Preparing the embedding layer"""


def load_embeddings(word_index, GLOVE_FILE_PATH):
    # Load glove word embeddings
    embeddings_index = {}
    f = open(GLOVE_FILE_PATH, 'r', encoding='utf8')
    for line in f:
        # each line starts with a word; rest of the line is the vector
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    f.close()
    print(f'Found {len(embeddings_index)} word vectors in glove file.')

    # Now use embedding_index dictionary and our word_index 
    # to compute our embedding matrix
    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))
    for word, i in word_index.items():
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            # words not found in embedding index will be all-zeros.
            embedding_matrix[i] = embedding_vector
    print("embedding_matrix shape:", np.shape(embedding_matrix))

    # load pre-trained word embeddings into an Embedding layer
    # note that we set trainable = False so as to keep the embeddings fixed
    embedding_layer = Embedding(len(word_index) + 1,
                                EMBEDDING_DIM,
                                weights=[embedding_matrix],
                                input_length=MAX_SEQUENCE_LENGTH,
                                trainable=False)
    return embedding_layer


"""# Load datasets"""

df = pd.read_csv(filepath_or_buffer=os.path.join(DATASET_PATH, TRAINING_FILE),
                 names=['article_id', 'title', 'articleContent', 'bias', 'hyperpartisan'],
                 dtype={'title': str},
                 nrows=N_TRAINING_SAMPLES)

df['title'] = df['title'].fillna(value=' ')
df.count()


def perform_cleaning(text):
    text = text.lower().strip()
    text = ' '.join(e for e in text.split())
    text = re.sub('[^A-Za-z0-9]+', ' ', text)
    return text


df['title'] = df['title'].map(perform_cleaning)
df['articleContent'] = df['articleContent'].map(perform_cleaning)

df.head()
# df.tail(5)

df_test = pd.read_csv(filepath_or_buffer=os.path.join(DATASET_PATH, TEST_FILE),
                      names=['article_id', 'title', 'articleContent', 'bias', 'hyperpartisan'],
                      nrows=N_TEST_SAMPLES
                      )
df_test['title'] = df_test['title'].fillna(value=' ')
df_test.count()

df_test['title'] = df_test['title'].map(perform_cleaning)
df_test['articleContent'] = df_test['articleContent'].map(perform_cleaning)
df_test.tail(5)

print(df['hyperpartisan'].value_counts())
print(df_test['hyperpartisan'].value_counts())

"""# Binary classifier (Biased / Unbiased)

## Separate labels from features
"""

X = df.articleContent.values
y_bias = df.hyperpartisan.values
y_bias_kind = df.bias.values

X_test = df_test.articleContent.values
y_test_bias = df_test.hyperpartisan.values
y_test_bias_kind = df_test.bias.values

NUM_CLASSES_BIAS = len(np.unique(y_bias))
NUM_CLASSES_BIAS_KIND = len(np.unique(y_bias_kind))

print(y_bias[:5])
print(y_bias_kind[:5])

"""## Tokenize data"""

X, word_index = tokenize_trainingdata(X)
y_bias = to_categorical(y_bias, num_classes=NUM_CLASSES_BIAS)

X_test = tokenize_testdata(X_test)
y_test_bias = to_categorical(y_test_bias, num_classes=NUM_CLASSES_BIAS)

print(y_bias[:5])
print(reverse_to_categorical(y_bias[:5]))

X_train, X_validate, y_train_bias, y_validate_bias = train_test_split(X, y_bias,
                                                                      test_size=0.2,
                                                                      random_state=12)

"""## Compile model"""

loaded_embeddings = load_embeddings(word_index,
                                    f'{DATASET_PATH}/glove.6B.{EMBEDDING_DIM}d.txt')

input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')

embedding_layer = loaded_embeddings(input_layer)
embedding_layer = Dropout(0.5)(embedding_layer)

hidden_layer = LSTM(64, recurrent_dropout=0.5)(embedding_layer)
hidden_layer = Dropout(0.5)(hidden_layer)
output_layer = Dense(2, activation='softmax')(hidden_layer)

model = Model(input_layer, output_layer)
model.compile(loss='categorical_crossentropy',
              optimizer='adamax',
              metrics=['accuracy'])

print(model.summary())

"""## Fit model"""

CHECKPOINT_FILE = 'partial_lstm_binary.h5'

checkpoint = ModelCheckpoint(CHECKPOINT_FILE, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]
new_model = None

model.fit(X_train, y_train_bias,
          validation_data=(X_validate, y_validate_bias),
          epochs=25, batch_size=250,
          callbacks=callbacks_list)

# new_model = load_model(CHECKPOINT_FILE)

y_pred_bias_validate = model.predict(X_validate)
print(classification_report(np.argmax(y_validate_bias, axis=1),
                            np.argmax(y_pred_bias_validate, axis=1),
                            target_names=['unbiased', 'biased']))

"""## Predict"""

y_pred_bias = model.predict(X_test)
print(y_test_bias[:5])
print(y_pred_bias[:5])

print(classification_report(np.argmax(y_test_bias, axis=1),
                            np.argmax(y_pred_bias, axis=1),
                            target_names=['unbiased', 'biased']))

ax = plt.subplot()
cm = confusion_matrix(np.argmax(y_test_bias, axis=1), np.argmax(y_pred_bias, axis=1))
sns.heatmap(cm, annot=True, ax=ax, fmt='g')
ax.set_xlabel('Predicted')
ax.set_ylabel('True')

"""## Save model"""

model.save('lstm_binary.h5')

"""# Multiclass classifier (Kind of bias classifier)

## Separate labels from features
"""

print(y_bias_kind[:5])
print(y_test_bias_kind[:5])

"""## Encode labels"""

labelEncoder = LabelEncoder()
labelEncoder.fit(np.unique(y_bias_kind))
labelEncoder.classes_

y_bias_kind = labelEncoder.transform(y_bias_kind)
y_test_bias_kind = labelEncoder.transform(y_test_bias_kind)

print(y_bias_kind[:5])
print(y_test_bias_kind[:5])

# Inverse tranform labels
labelEncoder.inverse_transform(y_bias_kind[:5])

y_bias_kind = to_categorical(y_bias_kind, num_classes=NUM_CLASSES_BIAS_KIND)
y_test_bias_kind = to_categorical(y_test_bias_kind, num_classes=NUM_CLASSES_BIAS_KIND)

y_bias_kind[:5]

# TO get Reverse of to_categorical
print(reverse_to_categorical(y_bias_kind))

"""## Split into train and validate sets"""

X_train, X_validate, y_train_bias_kind, y_validate_bias_kind = train_test_split(X,
                                                                                y_bias_kind,
                                                                                test_size=0.2,
                                                                                random_state=12)

"""## Compile model"""

input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')

embedding_layer = loaded_embeddings(input_layer)
embedding_layer = Dropout(0.5)(embedding_layer)

hidden_layer = LSTM(64, recurrent_dropout=0.5)(embedding_layer)
hidden_layer = Dropout(0.5)(hidden_layer)
output_layer = Dense(NUM_CLASSES_BIAS_KIND, activation='softmax')(hidden_layer)

model = Model(input_layer, output_layer)
model.compile(loss='categorical_crossentropy',
              optimizer='adamax',
              metrics=['accuracy'])

print(model.summary())

"""## Fit model"""

CHECKPOINT_FILE = 'partial_lstm_multiclass.h5'

checkpoint = ModelCheckpoint(CHECKPOINT_FILE, monitor='loss', verbose=1,
                             save_best_only=True, mode='min')
callbacks_list = [checkpoint]

model.fit(X_train, y_train_bias_kind,
          validation_data=(X_validate, y_validate_bias_kind),
          epochs=25, batch_size=250,
          callbacks=callbacks_list)

# new_model = load_model(CHECKPOINT_FILE)

y_pred_bias_kind_validate = model.predict(X_validate)
print(classification_report(np.argmax(y_validate_bias_kind, axis=1),
                            np.argmax(y_pred_bias_kind_validate, axis=1),
                            target_names=labelEncoder.classes_))

"""## Predict"""

y_pred_bias_kind = model.predict(X_test)

y_test_bias_kind[:5]

y_pred_bias_kind[:5]

ax = plt.subplot()
cm = confusion_matrix(np.argmax(y_test_bias_kind, axis=1), np.argmax(y_pred_bias_kind, axis=1))
sns.heatmap(cm, annot=True, ax=ax)
ax.set_xlabel('Predicted')
ax.set_ylabel('True')

print(classification_report(np.argmax(y_test_bias_kind, axis=1),
                            np.argmax(y_pred_bias_kind, axis=1),
                            target_names=labelEncoder.classes_))

"""## Save model"""

model.save('lstm_multiclass.h5')

"""# Multitask learning 
 - task 1: biased/unbiased (binary)
 - task 2: kind of bias (multiclass)
"""

input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')

embedding_layer = loaded_embeddings(input_layer)
embedding_layer = Dropout(0.5)(embedding_layer)

hidden_layer = LSTM(64, recurrent_dropout=0.5)(embedding_layer)
hidden_layer = Dropout(0.5)(hidden_layer)

# task 1
output_bias = Dense(2, activation='softmax')(hidden_layer)

# task 2
output_bias_kind = Dense(5, activation='softmax')(hidden_layer)

model = Model(input_layer, [output_bias, output_bias_kind])

model.compile(loss='categorical_crossentropy',
              optimizer='adamax',
              metrics=['acc'])

print(model.summary())

"""## Fit model"""

CHECKPOINT_FILE = 'partial_lstm_multitask.h5'

checkpoint = ModelCheckpoint(CHECKPOINT_FILE, monitor='loss', verbose=1,
                             save_best_only=True, mode='min')
callbacks_list = [checkpoint]

model.fit(X_train, [y_train_bias, y_train_bias_kind],
          validation_data=(X_validate, [y_validate_bias, y_validate_bias_kind]),
          epochs=25, batch_size=250,
          callbacks=callbacks_list)

# model = load_model(CHECKPOINT_FILE)

y_pred_bias_validate, y_pred_bias_kind_validate = model.predict(X_validate)

print(classification_report(np.argmax(y_validate_bias, axis=1),
                            np.argmax(y_pred_bias_validate, axis=1),
                            target_names=['unbiased', 'biased']))

print(classification_report(np.argmax(y_validate_bias_kind, axis=1),
                            np.argmax(y_pred_bias_kind_validate, axis=1),
                            target_names=labelEncoder.classes_))

"""## Predict"""

y_pred_bias, y_pred_bias_kind = model.predict(X_test)

print(classification_report(np.argmax(y_test_bias, axis=1),
                            np.argmax(y_pred_bias, axis=1),
                            target_names=['unbiased', 'biased']))

print(classification_report(np.argmax(y_test_bias_kind, axis=1),
                            np.argmax(y_pred_bias_kind, axis=1),
                            target_names=labelEncoder.classes_))

ax = plt.subplot()
cm = confusion_matrix(np.argmax(y_test_bias, axis=1), np.argmax(y_pred_bias, axis=1))
sns.heatmap(cm, annot=True, ax=ax)
ax.set_xlabel('Predicted')
ax.set_ylabel('True')

ax = plt.subplot()
cm = confusion_matrix(np.argmax(y_test_bias_kind, axis=1), np.argmax(y_pred_bias_kind, axis=1))
sns.heatmap(cm, annot=True, ax=ax)
ax.set_xlabel('Predicted')
ax.set_ylabel('True')

"""## Save model"""

model.save('lstm_multitask.h5')
